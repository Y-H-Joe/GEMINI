# GEMINI
A light, fast, comprehensive and easy downstream pipeline for metagenomics whole genome sequencing

# GEMINI installation
1. download `GEMINI.yml`
2. create the conda environment `conda env create --name GEMINI --file GEMINI.yml `
3. activate the environment `conda activate GEMINI`
4. check you're using right humann `which humann`
5. download 1st humann databases `humann_databases --download chocophlan full /path/to/databases --update-config yes`
6. download 2nd humann databases `humann_databases --download uniref uniref90_diamond /path/to/databases --update-config yes`
7. download 3rd humann databases `humann_databases --download utility_mapping full /path/to/databases --update-config yes`
8. check your humann databases `cd /path/to/databases` then `ll`
9. check you're using right kaiju `which kaiju`
10. download kaiju databases ``
11. open `Snakefile.py`, modify the bwa,kaiju,python3,Rscript,...lefse_run parameters to the executable command lines in your environment. To make sure all command line works, please test the command line one by one in your linux shell.
12. test GEMINI. `cd parent/folder/of/GEMINI`. open and modify the `GEMINI/contig.tsv` to make sure the data_dir is right. then run `snakemake --cores 32 --verbose -s ./Snakefile.py --rerun-incomplete`.
13. You should see the outputs in `horsedonkey` folder. If no errors occured. then you're good to go.




# format of contig.tsv
| samples | bam_dir                                                  | assembly1   | assembly1_dir                                                | assembly2 | assembly2_dir                                                | group1 | group2      |
|---------|----------------------------------------------------------|-------------|--------------------------------------------------------------|-----------|--------------------------------------------------------------|--------|-------------|
| donkey1 | /analysis1/yihang_analysis/pipeline/data/bam/donkey1.bam | hd | /analysis1/yihang_analysis/pipeline/data/assembly/sample1.fa | hmdh      | /analysis1/yihang_analysis/pipeline/data/assembly/sample2.fa | donkey | horsedonkey |
| donkey2 | /analysis1/yihang_analysis/pipeline/data/bam/donkey2.bam | hd | /analysis1/yihang_analysis/pipeline/data/assembly/sample1.fa | hmdh      | /analysis1/yihang_analysis/pipeline/data/assembly/sample2.fa | donkey | horsedonkey |
| donkey3 | /analysis1/yihang_analysis/pipeline/data/bam/donkey3.bam | hd | /analysis1/yihang_analysis/pipeline/data/assembly/sample1.fa | hmdh      | /analysis1/yihang_analysis/pipeline/data/assembly/sample2.fa | donkey | horsedonkey |
| hinny1  | /analysis1/yihang_analysis/pipeline/data/bam/hinny1.bam  |             | /analysis1/yihang_analysis/pipeline/data/assembly/sample1.fa | hmdh      |                                                              |        | hinny       |
| hinny2  | /analysis1/yihang_analysis/pipeline/data/bam/hinny2.bam  |             | /analysis1/yihang_analysis/pipeline/data/assembly/sample1.fa | hmdh      |                                                              |        | hinny       |
| hinny3  | /analysis1/yihang_analysis/pipeline/data/bam/hinny3.bam  |             | /analysis1/yihang_analysis/pipeline/data/assembly/sample1.fa | hmdh      |                                                              |        | hinny       |
| horse1  | /analysis1/yihang_analysis/pipeline/data/bam/horse1.bam  | hd | /analysis1/yihang_analysis/pipeline/data/assembly/sample1.fa | hmdh      | /analysis1/yihang_analysis/pipeline/data/assembly/sample2.fa | horse  | horsedonkey |
| horse2  | /analysis1/yihang_analysis/pipeline/data/bam/horse2.bam  | hd | /analysis1/yihang_analysis/pipeline/data/assembly/sample1.fa | hmdh      | /analysis1/yihang_analysis/pipeline/data/assembly/sample2.fa | horse  | horsedonkey |
| horse3  | /analysis1/yihang_analysis/pipeline/data/bam/horse3.bam  | hd | /analysis1/yihang_analysis/pipeline/data/assembly/sample1.fa | hmdh      | /analysis1/yihang_analysis/pipeline/data/assembly/sample2.fa | horse  | horsedonkey |

The ***samples*** column contains the ids of each sample. Each sample id should be unique. The id can be seperated with '_', but ***not*** with any other separators. 

The ***bam_dir*** column contains the absolute directory of each ***bam***. The bam file should be generated by users, via aligning reads to assembly. Both PE and SE alignment are acceptable.

The ***assembly1*** column contains the name of 1st assembly. 

The ***assembly1_dir*** column contains the absolute directory of ***assembly1***.

The ***group1*** column contains the group name of the each sample.

For group comparison, we can have multiple ***assembly*** and ***group*** columns. Just change the suffix number of column, do ***not*** create 'interesting' columns like 'Assembly_3' or 'group-1', etc.

The same group id should retain in same column. We should not see 'hinny' in both ***group1*** column and ***group2*** column.

# group comparison starting from contig.tsv
In the above case. We have two references, hd and hmdh. For groups, we have 'donkey', 'horse', 'horsedonkey', 'hinny'.

The pipeline will pair-wise compare all of them. Which are, 
['donkey_vs_horse.hd','donkey_vs_horsedonkey.hd','donkey_vs_hinny.hd','horse_vs_horsedonky.hd','horse_vs_hinny.hd','horsedonkey_vs_hinny.hd',... and replace 'hd' with 'hmdh'].

Since the statistic part is ultra-fast, finishing the group comparison should not cost too much time.

# common questions
## When running humann3, get `No MetaPhlAn BowTie2 database found (--index option)!` error.
Basically, the cause is the wrong installation of humann3.
1. google it. Try `metaphlan —install`. Then re-run snakemake.
2. if 1. not work. check the error log, try to find this sentence `Expecting location /the/expection/location`; `cd` to the location, examine any file truncation/loss. If there be, remove all files in the `/the/expection/location`, re-run `metaphlan —install`. Then re-run snakemake.
